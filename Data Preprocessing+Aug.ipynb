{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation, File trasnfer, Data Cleaning, Data Preprocessing, Annotation Updates, Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Automold as am\n",
    "import Helpers as hp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented'"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\40\"))\n",
    "file_count = len(files)\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\FL-renamed\"))\n",
    "file_count = len(files)\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'D:\\Data 298 Images\\Merged Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen1 = ImageDataGenerator(rotation_range=20, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_iter1 = datagen1.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(600,800),\n",
    "        batch_size=32,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=['ped'],\n",
    "        save_format='jpg',\n",
    "        save_to_dir=r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped',\n",
    "        save_prefix='ARotation15-',\n",
    "        class_mode='categorical')\n",
    "plt.imshow(next(aug_iter1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(aug_iter1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'D:\\Data 298 Images\\Merged Dataset'\n",
    "datagen1 = ImageDataGenerator(rotation_range=35, fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_iter1 = datagen1.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(600,800),\n",
    "        batch_size=30,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=['ped'],\n",
    "        save_format='jpg',\n",
    "        save_to_dir=r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped',\n",
    "        save_prefix='ARotation30-',\n",
    "        class_mode='categorical')\n",
    "plt.imshow(next(aug_iter1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(aug_iter1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen2 = ImageDataGenerator(zoom_range=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_iter2 = datagen2.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(600,800),\n",
    "        batch_size=32,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=['ped'],\n",
    "        save_format='jpg',\n",
    "        save_to_dir=r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped',\n",
    "        save_prefix='AZoom',\n",
    "        class_mode='categorical')\n",
    "plt.imshow(next(aug_iter2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(aug_iter2)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen2 = ImageDataGenerator(zoom_range=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_iter2 = datagen2.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(600,800),\n",
    "        batch_size=32,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=['ped'],\n",
    "        save_format='jpg',\n",
    "        save_to_dir=r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped',\n",
    "        save_prefix='AZoom',\n",
    "        class_mode='categorical')\n",
    "plt.imshow(next(aug_iter2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(aug_iter2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(brightness_range=[0.2,1.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\extra\\try cars')\n",
    "path = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\extra'\n",
    "src_dir = glob.glob(r\"*.jpg\")\n",
    "\n",
    "aug_iter2 = datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(600,800),\n",
    "        batch_size=len(src_dir),\n",
    "        color_mode=\"rgb\",\n",
    "        classes=['try cars'],\n",
    "        save_format='jpg',\n",
    "        save_to_dir=r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\extra\\cars augmented',\n",
    "        class_mode='categorical')\n",
    "plt.imshow(next(aug_iter2)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ped1.JPG', 'ped10.JPG', 'ped11.JPG', 'ped12.JPG', 'ped13.JPG', 'ped14.JPG', 'ped15.JPG', 'ped16.JPG', 'ped17.JPG', 'ped18.JPG', 'ped19.JPG', 'ped2.JPG', 'ped20.JPG', 'ped21.JPG', 'ped22.JPG', 'ped23.JPG', 'ped24.JPG', 'ped25.JPG', 'ped26.JPG', 'ped27.JPG', 'ped28.JPG', 'ped29.JPG', 'ped3.JPG', 'ped30.JPG', 'ped31.jpg', 'ped32.jpg', 'ped33.jpg', 'ped4.JPG', 'ped5.JPG', 'ped6.JPG', 'ped7.JPG', 'ped8.JPG', 'ped9.JPG', 'pedestrian_1323822805.avi_image11.png', 'pedestrian_1323822805.avi_image12.png', 'pedestrian_1323822805.avi_image13.png', 'pedestrian_1323822805.avi_image14.png', 'pedestrian_1323822805.avi_image15.png', 'pedestrian_1323822805.avi_image16.png', 'pedestrian_1323822805.avi_image17.png', 'pedestrian_1323896929.avi_image14.png', 'pedestrian_1323896929.avi_image15.png', 'pedestrian_1323896929.avi_image16.png', 'pedestrian_1323896929.avi_image17.png', 'pedestrian_1323896929.avi_image18.png', 'pedestrian_1323896929.avi_image19.png', 'pedestrian_1323896929.avi_image21.png', 'roadmark_0368.jpg', 'roadmark_0369.jpg', 'roadmark_0370.jpg', 'roadmark_0371.jpg', 'roadmark_0372.jpg', 'roadmark_0373.jpg', 'roadmark_0374.jpg', 'roadmark_0375.jpg', 'roadmark_0376.jpg', 'roadmark_0377.jpg', 'roadmark_0378.jpg', 'roadmark_0379.jpg', 'roadmark_0380.jpg', 'roadmark_0381.jpg', 'roadmark_0382.jpg', 'roadmark_0383.jpg', 'roadmark_0384.jpg', 'roadmark_0385.jpg', 'roadmark_0476.jpg', 'roadmark_0477.jpg', 'roadmark_0478.jpg', 'roadmark_0479.jpg', 'roadmark_0480.jpg', 'roadmark_0481.jpg', 'roadmark_0482.jpg', 'roadmark_0483.jpg', 'roadmark_0484.jpg', 'roadmark_0485.jpg', 'roadmark_0486.jpg', 'roadmark_0487.jpg', 'roadmark_0488.jpg', 'roadmark_0489.jpg', 'roadmark_0490.jpg', 'roadmark_0491.jpg', 'roadmark_0492.jpg', 'roadmark_0493.jpg', 'roadmark_0494.jpg', 'roadmark_0495.jpg', 'roadmark_0496.jpg', 'roadmark_0497.jpg', 'roadmark_0498.jpg', 'roadmark_0499.jpg', 'roadmark_0500.jpg', 'roadmark_0501.jpg', 'roadmark_0502.jpg', 'roadmark_0503.jpg', 'roadmark_0504.jpg', 'roadmark_0505.jpg', 'roadmark_0506.jpg', 'roadmark_0507.jpg', 'roadmark_1298.jpg', 'roadmark_1299.jpg', 'roadmark_1385.jpg', 'roadmark_1401.jpg']\n"
     ]
    }
   ],
   "source": [
    "path=r'D:\\Data 298 Images\\Merged Dataset\\ped'\n",
    "\n",
    "from os import walk\n",
    "\n",
    "filenames = next(walk(path), (None, None, []))[2]\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filenames:\n",
    "    img = cv2.imread(r'D:\\Data 298 Images\\Merged Dataset\\ped\\\\' + i)\n",
    "    img = datagen2.random_transform(img , seed=None)\n",
    "    cv2.imwrite(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped\\\\Random'+i, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Transformations using Automold library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\train udacity\\bikers\\*.jpg'\n",
    "images= hp.load_images(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_images= am.augment_random(images[:], aug_types=['random_brightness'], volume='same')  ##all aug_types are applied in both images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(aug_images)):\n",
    "    cv2.imwrite(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\train udacity\\bikers augmented\\bikers-'+str(i)+'.jpg', cv2.cvtColor(aug_images[i], cv2.COLOR_RGB2BGR) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_images= am.augment_random(images[:], aug_types=['add_sun_flare','add_shadow'], volume='same')  ##all aug_types are applied in both images\n",
    "for i in range(len(aug_images)):\n",
    "    cv2.imwrite(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\FLR\\A2-'+str(i)+'.jpg', cv2.cvtColor(aug_images[i], cv2.COLOR_RGB2BGR) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File transfer to segment and renaming the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARotation15-_0_7085674.jpg', 'ARotation15-_10_8728655.jpg', 'ARotation15-_12_5572781.jpg', 'ARotation15-_18_7061316.jpg', 'ARotation15-_19_5466081.jpg', 'ARotation15-_23_479478.jpg', 'ARotation15-_28_5637328.jpg', 'ARotation15-_30_2643075.jpg', 'ARotation15-_33_1975560.jpg', 'ARotation15-_34_7507093.jpg', 'ARotation15-_36_634736.jpg', 'ARotation15-_43_7074539.jpg', 'ARotation15-_50_452030.jpg', 'ARotation15-_52_4693340.jpg', 'ARotation15-_53_232339.jpg', 'ARotation15-_57_4788550.jpg', 'ARotation15-_58_6679629.jpg', 'ARotation15-_60_5803422.jpg', 'ARotation15-_66_4534521.jpg', 'ARotation15-_6_4972846.jpg', 'ARotation15-_70_3066288.jpg', 'ARotation15-_73_3563001.jpg', 'ARotation15-_74_5037098.jpg', 'ARotation15-_79_7694751.jpg', 'ARotation15-_82_9015056.jpg', 'ARotation15-_85_1539165.jpg', 'ARotation15-_87_2248783.jpg', 'ARotation15-_90_970312.jpg', 'ARotation15-_92_6189727.jpg', 'ARotation15-_94_6521288.jpg', 'ARotation15-_99_3859810.jpg', 'ARotation15-_9_2853840.jpg', 'ARotation30-_14_9006263.jpg', 'ARotation30-_18_3471141.jpg', 'ARotation30-_19_2408694.jpg', 'ARotation30-_2_5865425.jpg', 'ARotation30-_32_5241204.jpg', 'ARotation30-_36_4934595.jpg', 'ARotation30-_38_3964545.jpg', 'ARotation30-_3_6964957.jpg', 'ARotation30-_40_5092356.jpg', 'ARotation30-_41_5823281.jpg', 'ARotation30-_43_8918231.jpg', 'ARotation30-_44_358815.jpg', 'ARotation30-_48_3833458.jpg', 'ARotation30-_49_7715747.jpg', 'ARotation30-_4_1221188.jpg', 'ARotation30-_51_7964942.jpg', 'ARotation30-_59_9881398.jpg', 'ARotation30-_5_127847.jpg', 'ARotation30-_61_7472076.jpg', 'ARotation30-_63_8310366.jpg', 'ARotation30-_64_7978035.jpg', 'ARotation30-_69_6318085.jpg', 'ARotation30-_79_4674169.jpg', 'ARotation30-_80_7915437.jpg', 'ARotation30-_85_3444117.jpg', 'ARotation30-_88_3342915.jpg', 'ARotation30-_90_5075791.jpg', 'ARotation30-_98_3083945.jpg', 'ARotation30-_99_1121374.jpg', 'ARotation30-_9_2320751.jpg', 'AZoom_0_6095343.jpg', 'AZoom_100_1171311.jpg', 'AZoom_10_9267185.jpg', 'AZoom_12_2458757.jpg', 'AZoom_13_8769715.jpg', 'AZoom_15_7076776.jpg', 'AZoom_17_6081013.jpg', 'AZoom_18_4645205.jpg', 'AZoom_18_9857048.jpg', 'AZoom_1_4535828.jpg', 'AZoom_21_1341641.jpg', 'AZoom_22_9418745.jpg', 'AZoom_23_145308.jpg', 'AZoom_25_6700185.jpg', 'AZoom_28_7062927.jpg', 'AZoom_28_7567556.jpg', 'AZoom_2_8765735.jpg', 'AZoom_30_9894641.jpg', 'AZoom_35_4206983.jpg', 'AZoom_36_8347347.jpg', 'AZoom_37_4640864.jpg', 'AZoom_38_7321604.jpg', 'AZoom_39_2153520.jpg', 'AZoom_3_645439.jpg', 'AZoom_40_541947.jpg', 'AZoom_41_3137921.jpg', 'AZoom_43_6616656.jpg', 'AZoom_48_4374669.jpg', 'AZoom_51_4574625.jpg', 'AZoom_52_4841103.jpg', 'AZoom_52_6825775.jpg', 'AZoom_55_1346989.jpg', 'AZoom_56_810915.jpg', 'AZoom_58_2431308.jpg', 'AZoom_58_7414594.jpg', 'AZoom_59_1527998.jpg', 'AZoom_59_7044000.jpg', 'AZoom_60_8855730.jpg', 'AZoom_62_2965436.jpg', 'AZoom_62_4361241.jpg', 'AZoom_64_5077593.jpg', 'AZoom_65_2756542.jpg', 'AZoom_66_5228604.jpg', 'AZoom_68_9035505.jpg', 'AZoom_71_1308728.jpg', 'AZoom_71_3073011.jpg', 'AZoom_73_1197424.jpg', 'AZoom_74_5278354.jpg', 'AZoom_77_4389248.jpg', 'AZoom_78_9285787.jpg', 'AZoom_79_204343.jpg', 'AZoom_80_7832628.jpg', 'AZoom_81_6643361.jpg', 'AZoom_83_847475.jpg', 'AZoom_84_9950734.jpg', 'AZoom_88_2607697.jpg', 'AZoom_89_9772287.jpg', 'AZoom_8_1635808.jpg', 'AZoom_92_3421685.jpg', 'AZoom_93_5340984.jpg', 'AZoom_93_8885465.jpg', 'AZoom_95_8127977.jpg', 'AZoom_98_8126089.jpg', 'AZoom_9_475207.jpg', 'Randomped1.JPG', 'Randomped10.JPG', 'Randomped11.JPG', 'Randomped12.JPG', 'Randomped13.JPG', 'Randomped14.JPG', 'Randomped15.JPG', 'Randomped16.JPG', 'Randomped17.JPG', 'Randomped18.JPG', 'Randomped19.JPG', 'Randomped2.JPG', 'Randomped20.JPG', 'Randomped21.JPG', 'Randomped22.JPG', 'Randomped23.JPG', 'Randomped24.JPG', 'Randomped25.JPG', 'Randomped26.JPG', 'Randomped27.JPG', 'Randomped28.JPG', 'Randomped29.JPG', 'Randomped3.JPG', 'Randomped30.JPG', 'Randomped31.jpg', 'Randomped32.jpg', 'Randomped33.jpg', 'Randomped4.JPG', 'Randomped5.JPG', 'Randomped6.JPG', 'Randomped7.JPG', 'Randomped8.JPG', 'Randomped9.JPG', 'Randompedestrian_1323822805.avi_image11.png', 'Randompedestrian_1323822805.avi_image12.png', 'Randompedestrian_1323822805.avi_image13.png', 'Randompedestrian_1323822805.avi_image14.png', 'Randompedestrian_1323822805.avi_image15.png', 'Randompedestrian_1323822805.avi_image16.png', 'Randompedestrian_1323822805.avi_image17.png', 'Randompedestrian_1323896929.avi_image14.png', 'Randompedestrian_1323896929.avi_image15.png', 'Randompedestrian_1323896929.avi_image16.png', 'Randompedestrian_1323896929.avi_image17.png', 'Randompedestrian_1323896929.avi_image18.png', 'Randompedestrian_1323896929.avi_image19.png', 'Randompedestrian_1323896929.avi_image21.png', 'Randomroadmark_0368.jpg', 'Randomroadmark_0369.jpg', 'Randomroadmark_0370.jpg', 'Randomroadmark_0371.jpg', 'Randomroadmark_0372.jpg', 'Randomroadmark_0373.jpg', 'Randomroadmark_0374.jpg', 'Randomroadmark_0375.jpg', 'Randomroadmark_0376.jpg', 'Randomroadmark_0377.jpg', 'Randomroadmark_0378.jpg', 'Randomroadmark_0379.jpg', 'Randomroadmark_0380.jpg', 'Randomroadmark_0381.jpg', 'Randomroadmark_0382.jpg', 'Randomroadmark_0383.jpg', 'Randomroadmark_0384.jpg', 'Randomroadmark_0385.jpg', 'Randomroadmark_0476.jpg', 'Randomroadmark_0477.jpg', 'Randomroadmark_0478.jpg', 'Randomroadmark_0479.jpg', 'Randomroadmark_0480.jpg', 'Randomroadmark_0481.jpg', 'Randomroadmark_0482.jpg', 'Randomroadmark_0483.jpg', 'Randomroadmark_0484.jpg', 'Randomroadmark_0485.jpg', 'Randomroadmark_0486.jpg', 'Randomroadmark_0487.jpg', 'Randomroadmark_0488.jpg', 'Randomroadmark_0489.jpg', 'Randomroadmark_0490.jpg', 'Randomroadmark_0491.jpg', 'Randomroadmark_0492.jpg', 'Randomroadmark_0493.jpg', 'Randomroadmark_0494.jpg', 'Randomroadmark_0495.jpg', 'Randomroadmark_0496.jpg', 'Randomroadmark_0497.jpg', 'Randomroadmark_0498.jpg', 'Randomroadmark_0499.jpg', 'Randomroadmark_0500.jpg', 'Randomroadmark_0501.jpg', 'Randomroadmark_0502.jpg', 'Randomroadmark_0503.jpg', 'Randomroadmark_0504.jpg', 'Randomroadmark_0505.jpg', 'Randomroadmark_0506.jpg', 'Randomroadmark_0507.jpg', 'Randomroadmark_1298.jpg', 'Randomroadmark_1299.jpg', 'Randomroadmark_1385.jpg', 'Randomroadmark_1401.jpg']\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped'\n",
    "new_folder = r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped-renamed'\n",
    "src_dir = 'D:\\Data 298 Images\\Merged Dataset\\Augmented\\\\ped\\\\' #get the current working dir\n",
    "os.mkdir('ped-renamed')\n",
    "\n",
    "from os import walk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "filenames = next(walk(path), (None, None, []))[2]\n",
    "print(filenames)\n",
    "i=0\n",
    "for file in filenames:\n",
    "    i+=1\n",
    "    os.listdir()\n",
    "\n",
    "    dest_dir = new_folder\n",
    "    src_file = os.path.join(src_dir, file)\n",
    "    shutil.copy(src_file,dest_dir) #copy the file to destination dir\n",
    "\n",
    "    dst_file = os.path.join(dest_dir, file)\n",
    "    new_dst_file_name = os.path.join(dest_dir, 'A'+str(i)+'.txt')\n",
    "\n",
    "    os.rename(dst_file, new_dst_file_name)#rename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ped1.JPG', 'ped10.JPG', 'ped11.JPG', 'ped12.JPG', 'ped13.JPG', 'ped14.JPG', 'ped15.JPG', 'ped16.JPG', 'ped17.JPG', 'ped18.JPG', 'ped19.JPG', 'ped2.JPG', 'ped20.JPG', 'ped21.JPG', 'ped22.JPG', 'ped23.JPG', 'ped24.JPG', 'ped25.JPG', 'ped26.JPG', 'ped27.JPG', 'ped28.JPG', 'ped29.JPG', 'ped3.JPG', 'ped30.JPG', 'ped31.jpg', 'ped32.jpg', 'ped33.jpg', 'ped4.JPG', 'ped5.JPG', 'ped6.JPG', 'ped7.JPG', 'ped8.JPG', 'ped9.JPG', 'pedestrian_1323822805.avi_image11.png', 'pedestrian_1323822805.avi_image12.png', 'pedestrian_1323822805.avi_image13.png', 'pedestrian_1323822805.avi_image14.png', 'pedestrian_1323822805.avi_image15.png', 'pedestrian_1323822805.avi_image16.png', 'pedestrian_1323822805.avi_image17.png', 'pedestrian_1323896929.avi_image14.png', 'pedestrian_1323896929.avi_image15.png', 'pedestrian_1323896929.avi_image16.png', 'pedestrian_1323896929.avi_image17.png', 'pedestrian_1323896929.avi_image18.png', 'pedestrian_1323896929.avi_image19.png', 'pedestrian_1323896929.avi_image21.png', 'roadmark_0368.jpg', 'roadmark_0369.jpg', 'roadmark_0370.jpg', 'roadmark_0371.jpg', 'roadmark_0372.jpg', 'roadmark_0373.jpg', 'roadmark_0374.jpg', 'roadmark_0375.jpg', 'roadmark_0376.jpg', 'roadmark_0377.jpg', 'roadmark_0378.jpg', 'roadmark_0379.jpg', 'roadmark_0380.jpg', 'roadmark_0381.jpg', 'roadmark_0382.jpg', 'roadmark_0383.jpg', 'roadmark_0384.jpg', 'roadmark_0385.jpg', 'roadmark_0476.jpg', 'roadmark_0477.jpg', 'roadmark_0478.jpg', 'roadmark_0479.jpg', 'roadmark_0480.jpg', 'roadmark_0481.jpg', 'roadmark_0482.jpg', 'roadmark_0483.jpg', 'roadmark_0484.jpg', 'roadmark_0485.jpg', 'roadmark_0486.jpg', 'roadmark_0487.jpg', 'roadmark_0488.jpg', 'roadmark_0489.jpg', 'roadmark_0490.jpg', 'roadmark_0491.jpg', 'roadmark_0492.jpg', 'roadmark_0493.jpg', 'roadmark_0494.jpg', 'roadmark_0495.jpg', 'roadmark_0496.jpg', 'roadmark_0497.jpg', 'roadmark_0498.jpg', 'roadmark_0499.jpg', 'roadmark_0500.jpg', 'roadmark_0501.jpg', 'roadmark_0502.jpg', 'roadmark_0503.jpg', 'roadmark_0504.jpg', 'roadmark_0505.jpg', 'roadmark_0506.jpg', 'roadmark_0507.jpg', 'roadmark_1298.jpg', 'roadmark_1299.jpg', 'roadmark_1385.jpg', 'roadmark_1401.jpg']\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\Data 298 Images\\Merged Dataset\\ped'\n",
    "new_folder = r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\ped-renamed'\n",
    "src_dir = 'D:\\Data 298 Images\\Merged Dataset\\\\ped\\\\' #get the current working dir\n",
    "i=227\n",
    "import shutil\n",
    "\n",
    "from os import walk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "filenames = next(walk(path), (None, None, []))[2]\n",
    "print(filenames)\n",
    "\n",
    "for file in filenames:\n",
    "    i+=1\n",
    "    os.listdir()\n",
    "\n",
    "    dest_dir = new_folder\n",
    "    src_file = os.path.join(src_dir, file)\n",
    "    shutil.copy(src_file,dest_dir) #copy the file to destination dir\n",
    "\n",
    "    dst_file = os.path.join(dest_dir, file)\n",
    "    new_dst_file_name = os.path.join(dest_dir, 'A'+str(i)+'.jpg')\n",
    "\n",
    "    os.rename(dst_file, new_dst_file_name)#rename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 720)\n",
      "(800, 600)\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\25\\A-0.jpg')\n",
    "new_image = image.resize((800, 600))\n",
    "new_image.save('image_400.jpg')\n",
    "\n",
    "print(image.size) # Output: (1920, 1280)\n",
    "print(new_image.size) # Output: (400, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\25\\A-0.jpg')\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.format) # Output: JPEG\n",
    "\n",
    "# The pixel format used by the image. Typical values are \"1\", \"L\", \"RGB\", or \"CMYK.\"\n",
    "print(image.mode) # Output: RGB\n",
    "\n",
    "# Image size, in pixels. The size is given as a 2-tuple (width, height).\n",
    "print(image.size) # Output: (1920, 1280)\n",
    "\n",
    "# Colour palette table, if any.\n",
    "print(image.palette) # Output: None\n",
    "\n",
    "image = Image.open('demo_image.jpg')\n",
    "image.save('new_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6803\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\all labels\"))\n",
    "file_count = len(files)\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6804\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\All images\"))\n",
    "file_count = len(files)\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename from 1 for each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6707\n",
      "['A1.jpg', 'A10.jpg', 'A100.jpg', 'A101.jpg', 'A102.jpg', 'A103.jpg', 'A104.jpg', 'A105.jpg', 'A106.jpg', 'A107.jpg', 'A108.jpg', 'A109.jpg', 'A11.jpg', 'A110.jpg', 'A111.jpg', 'A112.jpg', 'A113.jpg', 'A114.jpg', 'A115.jpg', 'A116.jpg', 'A117.jpg', 'A118.jpg', 'A119.jpg', 'A12.jpg', 'A120.jpg', 'A121.jpg', 'A122.jpg', 'A123.jpg', 'A124.jpg', 'A125.jpg', 'A126.jpg', 'A127.jpg', 'A128.jpg', 'A129.jpg', 'A13.jpg', 'A130.jpg', 'A131.jpg', 'A132.jpg', 'A133.jpg', 'A134.jpg', 'A135.jpg', 'A136.jpg', 'A137.jpg', 'A138.jpg', 'A139.jpg', 'A14.jpg', 'A140.jpg', 'A141.jpg', 'A142.jpg', 'A143.jpg', 'A144.jpg', 'A145.jpg', 'A146.jpg', 'A147.jpg', 'A148.jpg', 'A149.jpg', 'A15.jpg', 'A150.jpg', 'A151.jpg', 'A152.jpg', 'A153.jpg', 'A154.jpg', 'A155.jpg', 'A156.jpg', 'A157.jpg', 'A158.jpg', 'A159.jpg', 'A16.jpg', 'A160.jpg', 'A161.jpg', 'A162.jpg', 'A163.jpg', 'A164.jpg', 'A165.jpg', 'A166.jpg', 'A167.jpg', 'A168.jpg', 'A169.jpg', 'A17.jpg', 'A170.jpg', 'A171.jpg', 'A172.jpg', 'A173.jpg', 'A174.jpg', 'A175.jpg', 'A176.jpg', 'A177.jpg', 'A178.jpg', 'A179.jpg', 'A18.jpg', 'A180.jpg', 'A181.jpg', 'A182.jpg', 'A19.jpg', 'A2.jpg', 'A20.jpg', 'A21.jpg', 'A22.jpg', 'A23.jpg', 'A24.jpg', 'A25.jpg', 'A26.jpg', 'A27.jpg', 'A28.jpg', 'A29.jpg', 'A3.jpg', 'A30.jpg', 'A31.jpg', 'A32.jpg', 'A33.jpg', 'A34.jpg', 'A35.jpg', 'A36.jpg', 'A37.jpg', 'A38.jpg', 'A39.jpg', 'A4.jpg', 'A40.jpg', 'A41.jpg', 'A42.jpg', 'A43.jpg', 'A44.jpg', 'A45.jpg', 'A46.jpg', 'A47.jpg', 'A48.jpg', 'A49.jpg', 'A5.jpg', 'A50.jpg', 'A51.jpg', 'A52.jpg', 'A53.jpg', 'A54.jpg', 'A55.jpg', 'A56.jpg', 'A57.jpg', 'A58.jpg', 'A59.jpg', 'A6.jpg', 'A60.jpg', 'A61.jpg', 'A62.jpg', 'A63.jpg', 'A64.jpg', 'A65.jpg', 'A66.jpg', 'A67.jpg', 'A68.jpg', 'A69.jpg', 'A7.jpg', 'A70.jpg', 'A71.jpg', 'A72.jpg', 'A73.jpg', 'A74.jpg', 'A75.jpg', 'A76.jpg', 'A77.jpg', 'A78.jpg', 'A79.jpg', 'A8.jpg', 'A80.jpg', 'A81.jpg', 'A82.jpg', 'A83.jpg', 'A84.jpg', 'A85.jpg', 'A86.jpg', 'A87.jpg', 'A88.jpg', 'A89.jpg', 'A9.jpg', 'A90.jpg', 'A91.jpg', 'A92.jpg', 'A93.jpg', 'A94.jpg', 'A95.jpg', 'A96.jpg', 'A97.jpg', 'A98.jpg', 'A99.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\25+30\"))\n",
    "file_count = len(files)\n",
    "print(file_count)\n",
    "i = file_count\n",
    "src_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\yield-renamed\"\n",
    "\n",
    "filenames = next(walk(src_dir), (None, None, []))[2]\n",
    "print(filenames)\n",
    "\n",
    "for file in filenames:\n",
    "    i+=1\n",
    "    os.listdir()\n",
    "    \n",
    "    old_name = os.path.join(src_dir, file)\n",
    "    new_dst_file_name = os.path.join(src_dir, 'A'+str(i)+'.jpg')\n",
    "\n",
    "    os.rename(old_name, new_dst_file_name)\n",
    "    \n",
    "    shutil.copy(new_dst_file_name, path) #copy the file to destination dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    i+=1\n",
    "    os.listdir()\n",
    "\n",
    "    dest_dir = new_folder\n",
    "    src_file = os.path.join(src_dir, file)\n",
    "    shutil.copy(src_file,dest_dir) #copy the file to destination dir\n",
    "\n",
    "    dst_file = os.path.join(dest_dir, file)\n",
    "    new_dst_file_name = os.path.join(dest_dir, 'A'+str(i)+'.jpg')\n",
    "\n",
    "    os.rename(dst_file, new_dst_file_name)#rename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "\n",
    "rootdir= r'D:\\32 classes' #path of the original folder\n",
    "\n",
    "classes = ['all images including supplemental']\n",
    "\n",
    "for i in classes:\n",
    "\n",
    "    os.makedirs(rootdir +'\\\\train\\\\' + i)\n",
    "\n",
    "    os.makedirs(rootdir +'\\\\test\\\\' + i)\n",
    "\n",
    "    source = rootdir + '\\\\' + i\n",
    "\n",
    "    allFileNames = []\n",
    "    for file in os.listdir(source):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            allFileNames.append(file)\n",
    "    \n",
    "    np.random.shuffle(allFileNames)\n",
    "\n",
    "    test_ratio = 0.20\n",
    "\n",
    "    train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                          [int(len(allFileNames) * (1 - test_ratio))])\n",
    "\n",
    "    train_FileNames = [source+'\\\\'+ name for name in train_FileNames.tolist()]\n",
    "    test_FileNames = [source+'\\\\' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    for name in train_FileNames:\n",
    "      shutil.copy(name, rootdir +'\\\\train\\\\' + i)\n",
    "\n",
    "    for name in test_FileNames:\n",
    "      shutil.copy(name, rootdir +'\\\\test\\\\' + i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate same label files after splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "path, dirs, files = next(os.walk(r\"D:\\32 classes\\test\\all images including supplemental\"))\n",
    "dest_dir = r\"D:\\32 classes\\test\\all images including supplemental\"\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split('.')[0] + '.txt'\n",
    "    src_dir = r\"D:\\32 classes\\all_labels_32_classes\\\\\"+str(file_name)\n",
    "    shutil.copy(src_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renumbering the images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "\n",
    "src_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\\\\\renamed all\\\\annotations\\\\L_yolo\"\n",
    "starting = 3771\n",
    "\n",
    "new_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\\\\\renamed all\\\\annotations\\\\all labels\"\n",
    "\n",
    "filenames = next(walk(src_dir), (None, None, []))[2]\n",
    "\n",
    "\n",
    "for file in filenames:\n",
    "    \n",
    "    image_number = int(file.split('A')[1].split('.')[0]) + starting\n",
    "    \n",
    "    new_file = 'A'+str(image_number)+'.txt'\n",
    "    \n",
    "    old_name = os.path.join(src_dir, file)\n",
    "    new_name = os.path.join(src_dir, new_file)\n",
    "    os.rename(old_name , new_name)\n",
    "    \n",
    "    shutil.copy(new_name, new_dir) #copy the file to destination dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path, dirs, files = next(os.walk(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\25+30\"))\n",
    "file_count = len(files)\n",
    "print(file_count)\n",
    "i = file_count\n",
    "src_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\yield-renamed\"\n",
    "\n",
    "filenames = next(walk(src_dir), (None, None, []))[2]\n",
    "print(filenames)\n",
    "\n",
    "for file in filenames:\n",
    "    i+=1\n",
    "    os.listdir()\n",
    "    \n",
    "    old_name = os.path.join(src_dir, file)\n",
    "    new_dst_file_name = os.path.join(src_dir, 'A'+str(i)+'.jpg')\n",
    "\n",
    "    os.rename(old_name, new_dst_file_name)\n",
    "    \n",
    "    shutil.copy(new_dst_file_name, path) #copy the file to destination dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnn = int(aaa.split('A')[1].split('.')[0])\n",
    "nnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A100.txt'"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_name = 'A'+str(nnn)+'.txt'\n",
    "new_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipped numbers of images that need to be excluded for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "\n",
    "src_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\all labels\"\n",
    "\n",
    "filenames = next(walk(src_dir), (None, None, []))[2]\n",
    "\n",
    "image_numbers = []\n",
    "for file in filenames:\n",
    "    \n",
    "    image_number = int(file.split('A')[1].split('.')[0]) \n",
    "    \n",
    "    image_numbers.append(image_number)\n",
    "\n",
    "sorted_image_numbers = sorted(image_numbers)\n",
    "\n",
    "check = 0\n",
    "\n",
    "for number in sorted_image_numbers:\n",
    "    if number != check+1:\n",
    "        print(number)\n",
    "    \n",
    "    check = number\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File differences in two folders to confirm no images is missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "\n",
    "src_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\renamed all\\\\images\\\\all images numbered\"\n",
    "\n",
    "\n",
    "filenames_in_labels = next(walk(src_dir), (None, None, []))[2]\n",
    "\n",
    "src_dir = \"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\renamed all\\\\annotations\\\\all labels\"\n",
    "filenames_in_images = next(walk(src_dir), (None, None, []))[2]\n",
    "\n",
    "image_numbers = []\n",
    "for file in filenames_in_labels:\n",
    "    \n",
    "    image_number = file.split('.')[0] \n",
    "    \n",
    "    image_numbers.append(image_number)\n",
    "\n",
    "image_numbers2 = []\n",
    "\n",
    "for file in filenames_in_images:\n",
    "    \n",
    "    image_number = file.split('.')[0] \n",
    "    \n",
    "    image_numbers2.append(image_number)\n",
    "print(sorted(list(set(image_numbers) - set(image_numbers2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data 298 Images\\Merged Dataset\\Augmented\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\Data 298 Images\\Merged Dataset\\Augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the label numbers in all the YOLO txt annotation files after reading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from os import walk\n",
    "\n",
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\traffic sign\\Traffic signs data with YOLO format 4 classes\\ts\\ts')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\train roadmarks\\train\\labels')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if new_line[0]=='0':\n",
    "            new_line[0]='28'\n",
    "        elif new_line[0]=='1':\n",
    "            new_line[0]='29'\n",
    "        elif new_line[0]=='2':\n",
    "            new_line[0]='30'\n",
    "        elif new_line[0]=='3':\n",
    "            new_line[0]='31'\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks\\test\\labels - Copy')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if int(new_line[0]) in range(0,23):\n",
    "            new_line[0] = str(int(new_line[0]) + 80)\n",
    "        \n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the  Udacity dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename udacity images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "from os import walk\n",
    "\n",
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\export')\n",
    "\n",
    "filenames = glob.glob(r\"*.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "src_dir = r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\export\"\n",
    "\n",
    "new_dir = r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\selected\"\n",
    "\n",
    "\n",
    "for file in filenames:\n",
    "    \n",
    "    try:\n",
    "        new_file = file.split('_jpg')[0] + '.jpg'\n",
    "\n",
    "        old_name = os.path.join(src_dir, file)\n",
    "        new_name = os.path.join(src_dir, new_file)\n",
    "        os.rename(old_name , new_name)\n",
    "        shutil.copy(new_name, new_dir) #copy the file to destination dir\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    filenames = glob.glob(r\"*.txt\")\n",
    "for file in filenames:\n",
    "    \n",
    "    try:\n",
    "        new_file = file.split('_jpg')[0] + '.txt'\n",
    "\n",
    "        old_name = os.path.join(src_dir, file)\n",
    "        new_name = os.path.join(src_dir, new_file)\n",
    "        os.rename(old_name , new_name)\n",
    "        shutil.copy(new_name, new_dir) #copy the file to destination dir\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\selected\")\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if new_line[0]=='0':\n",
    "            new_line[0]='28'\n",
    "        elif new_line[0]=='1':\n",
    "            new_line[0]='29'\n",
    "        elif new_line[0]=='2':\n",
    "            new_line[0]='30'\n",
    "        elif new_line[0]=='3':\n",
    "            new_line[0]='31'\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks\\test\\test')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if int(new_line[0]) in range(80,103):\n",
    "            new_line[0] = str(int(new_line[0]) + 57 - 80)\n",
    "        \n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Udacity Labels by changing label numbers in all txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\try')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if new_line[0] in '102':\n",
    "            new_line[0]='3'\n",
    "        \n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "\n",
    "cur_dir = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\selected'\n",
    "dest_dir = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\cars'\n",
    "os.chdir(cur_dir)\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if new_line[0] == '1':\n",
    "            cur_file = os.path.join(cur_dir, file)\n",
    "            shutil.copy(cur_file, dest_dir)\n",
    "\n",
    "            img_file = file.split('.')[0] + '.jpg'\n",
    "            cur_img_file = os.path.join(cur_dir, img_file)\n",
    "            shutil.copy(cur_img_file, dest_dir)\n",
    "    \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    fin.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select few images from whole udacity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "\n",
    "rootdir= r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet' #path of the original folder\n",
    "\n",
    "classes = ['cars']\n",
    "\n",
    "for i in classes:\n",
    "\n",
    "    os.makedirs(rootdir +'\\\\extra\\\\' + i)\n",
    "\n",
    "    os.makedirs(rootdir +'\\\\all class\\\\' + i)\n",
    "\n",
    "    source = rootdir + '\\\\' + i\n",
    "\n",
    "    allFileNames = []\n",
    "    for file in os.listdir(source):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            allFileNames.append(file)\n",
    "    \n",
    "    np.random.shuffle(allFileNames)\n",
    "\n",
    "    test_ratio = 0.05\n",
    "\n",
    "    train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                          [int(len(allFileNames) * (1 - test_ratio))])\n",
    "\n",
    "    train_FileNames = [source+'\\\\'+ name for name in train_FileNames.tolist()]\n",
    "    test_FileNames = [source+'\\\\' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    for name in train_FileNames:\n",
    "      shutil.copy(name, rootdir +'\\\\extra\\\\' + i)\n",
    "\n",
    "    for name in test_FileNames:\n",
    "      shutil.copy(name, rootdir +'\\\\all class\\\\' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "path, dirs, files = next(os.walk(r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\cars\"))\n",
    "dest_dir = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\cars'\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split('.')[0] + '.txt'\n",
    "    src_dir = r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\cars\\\\\"+str(file_name)\n",
    "    shutil.copy(src_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train test for udacity selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "\n",
    "rootdir= r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class' #path of the original folder\n",
    "\n",
    "classes = ['cars','bikers','pedestrian','traffic lights','trucks']\n",
    "\n",
    "for i in classes:\n",
    "\n",
    "    os.makedirs(rootdir +'\\\\train\\\\' + i)\n",
    "\n",
    "    os.makedirs(rootdir +'\\\\test\\\\' + i)\n",
    "\n",
    "    source = rootdir + '\\\\' + i\n",
    "\n",
    "    allFileNames = []\n",
    "    for file in os.listdir(source):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            allFileNames.append(file)\n",
    "    \n",
    "    np.random.shuffle(allFileNames)\n",
    "\n",
    "    test_ratio = 0.20\n",
    "\n",
    "    train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                          [int(len(allFileNames) * (1 - test_ratio))])\n",
    "\n",
    "    train_FileNames = [source+'\\\\'+ name for name in train_FileNames.tolist()]\n",
    "    test_FileNames = [source+'\\\\' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    for name in train_FileNames:\n",
    "      shutil.copy(name, rootdir +'\\\\train\\\\' + i)\n",
    "\n",
    "    for name in test_FileNames:\n",
    "      shutil.copy(name, rootdir +'\\\\test\\\\' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "path, dirs, files = next(os.walk(r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test\\bikers\"))\n",
    "dest_dir = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test\\bikers'\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.split('.')[0] + '.txt'\n",
    "    src_dir = r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\bikers\\\\\"+str(file_name)\n",
    "    shutil.copy(src_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test\\pedestrian')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if new_line[0] == '0':\n",
    "            new_line[0]='23'\n",
    "        \n",
    "        elif new_line[0] == '1':\n",
    "            new_line[0]='24'\n",
    "        elif new_line[0] == '2':\n",
    "            new_line[0]='25'\n",
    "        elif new_line[0] == '3':\n",
    "            new_line[0]='26'\n",
    "        elif new_line[0] == '10':\n",
    "            new_line[0]='27'\n",
    "        \n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks + udacity\\test\\test')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if int(new_line[0]) in range(0,28):\n",
    "            new_line[0] = str(int(new_line[0]) + 52)\n",
    "        \n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras \n",
    "\n",
    "import glob,os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img,img_to_array, load_img\n",
    "\n",
    "img_path = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\extra\\try cars'\n",
    "outpath = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\extra\\cars augmented'\n",
    "\n",
    "filenames = glob.glob(img_path + \"\\*.jpg\",recursive=True)\n",
    "\n",
    "\n",
    "for img in filenames:\n",
    "\n",
    "    \n",
    "    src_fname, ext = os.path.splitext(img) \n",
    "    datagen = ImageDataGenerator(brightness_range=[0.2,1.8])\n",
    "\n",
    "\n",
    "    img = load_img(img)\n",
    "\n",
    "    x = img_to_array(img)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "\n",
    "    img_name = src_fname.split(\"\\\\\")[-1].rsplit('_', 1)[0]\n",
    "    print(img_name)\n",
    "    \n",
    "\n",
    "\n",
    "    a = datagen.flow(x,target_size = (600,800), batch_size=1, save_to_dir = outpath, \n",
    "                               save_prefix = img_name, save_format='jpg')\n",
    "    plt.imshow(next(a)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in filenames:\n",
    "\n",
    "    if \"DS_Store\" in img: continue\n",
    "    src_fname, ext = os.path.splitext(img) \n",
    "\n",
    "    datagen = ImageDataGenerator(rotation_range = 10,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            fill_mode='constant',cval=0.0)\n",
    "    img = load_img(img)\n",
    "\n",
    "    x = img_to_array(img)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "\n",
    "    img_name = src_fname.split('/')[-1]\n",
    "    new_dir = os.path.join(outpath, src_fname.split('/')[-1].rsplit('-', 1)[0])\n",
    "    if not os.path.lexists(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    #save_fname = os.path.join(new_dir, os.path.basename(img_name))\n",
    "    save_fname = new_dir\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow (x, batch_size=1, save_to_dir = save_fname, \n",
    "                               save_prefix = img_name, save_format='jpg'):\n",
    "        i+=1\n",
    "        if i>51:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\OIDToolKit\\OIDv4_ToolKit\\OID\\Dataset\\train\\Car')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        \n",
    "        if int(new_line[0])==23:\n",
    "            new_line[0] = '24'\n",
    "        \n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "            print(new_line)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks + udacity\\test\\test - old backup 0-27 Copy')\n",
    "PATH = r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks + udacity\\test\\test - old backup 0-27 Copy'\n",
    "Copy_to_path = r\"D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks + udacity\\test\\test - old backup 0-27 Copy\\\\\"\n",
    "\n",
    "for filename in glob.glob(r\"*.jpg\"):\n",
    "    img = Image.open(os.path.join(PATH, filename)) # images are color images\n",
    "    img = img.resize((800,600), Image.ANTIALIAS)\n",
    "    img.save(Copy_to_path+filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\train less images\\obj')\n",
    "PATH = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\train less images\\obj'\n",
    "Copy_to_path = r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\train less images\\obj\\\\\"\n",
    "\n",
    "for filename in glob.glob(r\"*.jpg\"):\n",
    "    img = Image.open(os.path.join(PATH, filename)) # images are color images\n",
    "    img = img.resize((800,600), Image.ANTIALIAS)\n",
    "    img.save(Copy_to_path+filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "os.chdir(r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test less images\\test less all')\n",
    "PATH = r'D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test less images\\test less all'\n",
    "Copy_to_path = r\"D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test less images\\test less all\\\\\"\n",
    "\n",
    "for filename in glob.glob(r\"*.jpg\"):\n",
    "    img = Image.open(os.path.join(PATH, filename)) # images are color images\n",
    "    img = img.resize((800,600), Image.ANTIALIAS)\n",
    "    img.save(Copy_to_path+filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\298 Datasets\\vehicle datasets\\Udacity Self Driving Car.v2-fixed-large.darknet\\all class\\test less images\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\train roadmarks + udacity\\train\\obj 52-79 - Copy')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        if int(new_line[0])==75:\n",
    "            new_line[0] = '1'\n",
    "        elif int(new_line[0])==76:\n",
    "            new_line[0] = '2'\n",
    "        elif int(new_line[0])==77:\n",
    "            new_line[0] = '0'\n",
    "        elif int(new_line[0])==78:\n",
    "            new_line[0] = '9'\n",
    "        elif int(new_line[0])==79:\n",
    "            new_line[0] = '7'\n",
    "            \n",
    "        elif int(new_line[0]) in range(52,75):\n",
    "            pass\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "            print(new_line)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(r'D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks + udacity\\test\\test 52-79 - Copy')\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        new_line = line.strip().split(' ')\n",
    "        if int(new_line[0])==75:\n",
    "            new_line[0] = '1'\n",
    "        elif int(new_line[0])==76:\n",
    "            new_line[0] = '2'\n",
    "        elif int(new_line[0])==77:\n",
    "            new_line[0] = '0'\n",
    "        elif int(new_line[0])==78:\n",
    "            new_line[0] = '9'\n",
    "        elif int(new_line[0])==79:\n",
    "            new_line[0] = '7'\n",
    "            \n",
    "        elif int(new_line[0]) in range(52,75):\n",
    "            pass\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(file)\n",
    "            print(new_line)\n",
    "        spaced_line = \" \".join(new_line)\n",
    "        new_lines.append(spaced_line)\n",
    "    \n",
    "\n",
    "    fout = open(file,\"wt\")\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data 298 Images\\Merged Dataset\\Augmented\\renamed all\\images\\test roadmarks + udacity\\test\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "os.chdir(r\"D:\\32 classes\\labels_my-project-yolo\")\n",
    "\n",
    "src_dir = glob.glob(r\"*.txt\")\n",
    "\n",
    "for file in src_dir:\n",
    "\n",
    "    fin = open(file, \"rt\")\n",
    "\n",
    "    new_lines = []\n",
    "    for line in fin:\n",
    "        \n",
    "        \n",
    "        new_lines.append(line.strip())\n",
    "    \n",
    "\n",
    "    fout = open(r\"D:\\32 classes\\all labels\\\\\"+str(file),\"a\")\n",
    "    fout.write('\\n')\n",
    "    fout.write('\\n'.join(str(line) for line in new_lines))\n",
    "\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "os.chdir(r\"D:\\32 classes\\train\\all images including supplemental\")\n",
    "\n",
    "src_folder = r\"D:\\32 classes\\train\\all images including supplemental\\\\\"\n",
    "dest_folder = r\"D:\\32 classes\\train\\train images\"\n",
    "\n",
    "\n",
    "src_dir = glob.glob(r\"*.jpg\")\n",
    "\n",
    "for file in src_dir:\n",
    "    shutil.copy(src_folder+str(file),dest_folder)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_boxes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open(r\"D:\\Ensemble\\result.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = json.load(file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert JSON files to txt for checking mAP and drawing graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "files = glob.glob(r\"D:\\effdet1\\test stage2 effdet\\Stage 2 YOLO txt\\*.json\")\n",
    "\n",
    "\n",
    "for i in files:\n",
    "\n",
    "    json_file = open(i)\n",
    "\n",
    "    content = json.load(json_file)[0]\n",
    "\n",
    "    labels = []\n",
    "    confidences = []\n",
    "    boxes=[]\n",
    "    \n",
    "    file_name = i.split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    file = r\"D:\\effdet1\\test stage2 effdet\\Stage 2 YOLO txt\\{}.txt\".format(file_name)\n",
    "    with open(file,'wt') as file:\n",
    "\n",
    "        for i in range(len(content['objects'])):\n",
    "            detection = content['objects'][i]\n",
    "            label = detection['class_id']\n",
    "            labels.append(label)\n",
    "\n",
    "            confidence = detection['confidence']\n",
    "            confidences.append(confidence)\n",
    "\n",
    "            box = detection['relative_coordinates']\n",
    "            x,y,w,h = box['center_x'], box['center_y'], box['width'], box['height']\n",
    "            \n",
    "            file.write(\"{} {} {} {} {} {}\\n\".format(label, confidence, x, y, w, h))\n",
    "\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open(r\"D:\\sample.json\") \n",
    "content_eff = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels_list = []\n",
    "scores_list = []\n",
    "boxes_list = []\n",
    "labels = []\n",
    "confidences = []\n",
    "boxes=[]\n",
    "\n",
    "# YOLO\n",
    "\n",
    "for i in range(len(content['objects'])):\n",
    "    detection = content['objects'][i]\n",
    "    label = detection['class_id']\n",
    "    labels.append(label)\n",
    "\n",
    "    confidence = detection['confidence']\n",
    "    confidences.append(confidence)\n",
    "\n",
    "    box = detection['relative_coordinates']\n",
    "    x1,y1,x2,y2 = box['center_x']-box['width']/2,box['center_y']-box['height']/2,box['center_x']+box['width']/2,box['center_y']+box['height']/2\n",
    "    boxes.append(list(np.around(np.array([x1,y1,x2,y2]),2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# in the new for loop of models\n",
    "labels_list.append(labels)  \n",
    "scores_list.append(confidences)\n",
    "boxes_list.append(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientDet\n",
    "labels_list = []\n",
    "scores_list = []\n",
    "boxes_list = []\n",
    "import json\n",
    "file = open(r\"D:\\sample.json\") \n",
    "content_eff = json.load(file)\n",
    "\n",
    "labels_eff = []\n",
    "confidences_eff = []\n",
    "boxes_eff = []\n",
    "\n",
    "effToOriginal = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:9, 7:10, 8:13, 9:11, 10:12, 11:14, 12:17, 13:6, 14:23, 15:7, 16:24, 17:8, 18:15, 19:31, 20:25, 21:16, 22:18,\n",
    " 23:30, 24:19, 25:20, 26:28, 27:26, 28:27, 29:21, 30:22, 31:29}\n",
    "\n",
    "for i in content_eff:\n",
    "    box_eff = content_eff[i][0]\n",
    "    boxes_eff.append(list(np.around(np.array(box_eff),2)))\n",
    "\n",
    "    confidence_eff = content_eff[i][1]\n",
    "    confidences_eff.append(confidence_eff)\n",
    "    \n",
    "    label_eff = content_eff[i][2]\n",
    "    labels_eff.append(int(label_eff))\n",
    "\n",
    "    \n",
    "labels_list.append(labels_eff)\n",
    "scores_list.append(confidences_eff)\n",
    "boxes_list.append(boxes_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the input xml file and read data in form of python dictionary using xmltodict module\n",
    "\n",
    "\n",
    "import json\n",
    "import xmltodict\n",
    " \n",
    "import glob\n",
    "xmlfiles = glob.glob(\"D:/Ensemble/xml/*.xml\")\n",
    "for i in xmlfiles:\n",
    "\n",
    "    with open(i) as xml_file:\n",
    "\n",
    "        data_dict = xmltodict.parse(xml_file.read())\n",
    "        xml_file.close()\n",
    "\n",
    "        # generate the object using json.dumps()\n",
    "        # corresponding to json data\n",
    "\n",
    "        json_data = json.dumps(data_dict)\n",
    "        JSONname = (i.split(\"\\\\\")[-1]).split('.')[0]\n",
    "        \n",
    "        # Write the json data to output\n",
    "        # json file\n",
    "        \n",
    "        with open(\"D:/Ensemble/XmlToJson/\"+str(JSONname)+\".json\", \"w\") as json_file:\n",
    "            json_file.write(json_data)\n",
    "            json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelToname = {0:'25',1:'30',2:'35',3:'40',4:'45',5:'50',6:'bike',7:'bus',8:'diamond',9:'F',10:'FL',\n",
    "                11:'FR',12:'KC',13:'FLR',14:'L',15:'ped',16:'rail',17:'R',18:'school',19:'signal',20:'stop',21:'xing',\n",
    "               22:'yield',23:'biker',24:'car',25:'pedestrian',26:'traffic light',27:'truck',28:'stop_sign',29:'yield_sign',\n",
    "               30:'school_sign',31:'ped_sign'}\n",
    "\n",
    "labels_list = []\n",
    "scores_list = []\n",
    "boxes_list = []\n",
    "\n",
    "with open(r\"D:\\Ensemble\\XmlToJson\\A511_SSD.json\",'r') as file:\n",
    "    content_ssd = json.load(file)\n",
    "    boxes_ssd = []\n",
    "    labels_ssd = []\n",
    "    confidences_ssd = []\n",
    "    width, height = content_ssd['annotation']['size']['width'],content_ssd['annotation']['size']['height'] \n",
    "    objects = content_ssd['annotation']['object']\n",
    "    print(width,height)\n",
    "    for i in objects:\n",
    "        print(i)\n",
    "        label_ssd = i['name']\n",
    "        \n",
    "        def get_key(val):\n",
    "            for key, value in labelToname.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "            return \"key doesn't exist\"\n",
    "        \n",
    "        label_ssd = get_key(i['name'])\n",
    "        labels_ssd.append(label_ssd)\n",
    "        \n",
    "        \n",
    "        xmin, ymin, xmax, ymax = i['bndbox']['xmin'], i['bndbox']['ymin'], i['bndbox']['xmax'], i['bndbox']['ymax'] \n",
    "        x1, y1, x2, y2 = int(xmin)/int(width), int(ymin)/int(height), int(xmax)/int(width), int(ymax)/int(height)\n",
    "        box_ssd = [x1, y1, x2, y2]\n",
    "        boxes_ssd.append(list(np.around(np.array(box_ssd),2)))\n",
    "        \n",
    "        confidences_ssd.append(0.65)\n",
    "\n",
    "labels_list.append(labels_ssd)\n",
    "scores_list.append(confidences_ssd)\n",
    "boxes_list.append(boxes_ssd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "weights = [1]\n",
    "\n",
    "iou_thr = 0.5\n",
    "skip_box_thr = 0.0001\n",
    "sigma = 0.1\n",
    "\n",
    "boxes, scores, labels = nms(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr)\n",
    "boxes, scores, labels = soft_nms(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, sigma=sigma, thresh=skip_box_thr)\n",
    "boxes, scores, labels = non_maximum_weighted(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "labels = [round(a) for a in labels]\n",
    "boxes, scores, labels = [boxes.tolist()], [scores.tolist()], [labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign label name from number to show on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelToname = {0:'25',1:'30',2:'35',3:'40',4:'45',5:'50',6:'bike',7:'bus',8:'diamond',9:'F',10:'FL',\n",
    "                11:'FR',12:'KC',13:'FLR',14:'L',15:'ped',16:'rail',17:'R',18:'school',19:'signal',20:'stop',21:'xing',\n",
    "               22:'yield',23:'biker',24:'car',25:'pedestrian',26:'traffic light',27:'truck',28:'stop_sign',29:'yield_sign',\n",
    "               30:'school_sign',31:'ped_sign'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign color to each oof the 32 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = []\n",
    "for i in range(32):\n",
    "    color_list.append(list(np.random.random(size=3) * 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate bounding boxes on images passed through ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(im, name='image'):\n",
    "    cv2.imshow(name, im.astype(np.uint8))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def show_boxes(boxes_list, scores_list, labels_list):\n",
    "    thickness = 5\n",
    "    \n",
    "    \n",
    "    img = cv2.imread(\"D://images1.jpg\", cv2.IMREAD_COLOR)\n",
    "    \n",
    "    \n",
    "    for i in range(len(boxes_list)):\n",
    "        for j in range(len(boxes_list[i])):\n",
    "            x1 = int(img.shape[1] * boxes_list[i][j][0])\n",
    "            y1 = int(img.shape[0] * boxes_list[i][j][1])\n",
    "            x2 = int(img.shape[1] * boxes_list[i][j][2])\n",
    "            y2 = int(img.shape[0] * boxes_list[i][j][3])\n",
    "            lbl = labels_list[i][j]\n",
    "\n",
    "            \n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color_list[lbl] ,int(thickness * scores_list[i][j]))\n",
    "            cv2.putText(img, labelToname[lbl], (x1,y1), cv2.FONT_HERSHEY_SIMPLEX, 1, color_list[lbl],thickness=2, lineType=cv2.LINE_AA)\n",
    "    show_image(img)\n",
    "    cv2.imwrite('D://ensembling//STOP2-1.jpg', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_boxes(boxes, scores, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Videos to Frames to pass into models to get txt output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vidcap = cv2.VideoCapture(r'D:/10sec.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "    cv2.imwrite(\"D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\VideoToImages\\\\frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "#     print('Read a new frame: ', success)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Images to Videos after bounding boxes are generated on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "\n",
    "img_array = []\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "                     \n",
    "for filename in sorted(glob.glob('D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\VideoToImages\\\\*.jpg'),key=numericalSort):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter('D:\\\\Data 298 Images\\\\Merged Dataset\\\\Augmented\\\\VideoToImages\\\\project2.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
